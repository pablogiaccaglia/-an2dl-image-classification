{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# GDrive connection"
      ],
      "metadata": {
        "id": "bVGKQ8OKGqJ8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DtPN1GPqhk2h"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9LuTPiZoiDMo"
      },
      "outputs": [],
      "source": [
        "%cd /gdrive/My Drive/HMK1"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Installs"
      ],
      "metadata": {
        "id": "LPedLrndGv77"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ba-TKCJj6DpD"
      },
      "outputs": [],
      "source": [
        "!pip install tensorflow==2.10.1 # needed for ConvNextLarge model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nZDJvhu7xtNd"
      },
      "outputs": [],
      "source": [
        "!pip install keras-cv --upgrade # needed for cutout data augmentation"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "FrtZTjW0HAVR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8uc4_ZKZiD4X",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "454e0930-03aa-483f-98e3-a34b3ebcdc31"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.10.1\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os\n",
        "import random\n",
        "import scipy as sp\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "from keras import backend as K\n",
        "from keras.callbacks import *\n",
        "from PIL import Image\n",
        "\n",
        "tfk = tf.keras\n",
        "tfkl = tf.keras.layers\n",
        "print(tf.__version__)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "9f2dWCLwHB11"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Env Setup"
      ],
      "metadata": {
        "id": "jgNHqEB3HClH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3crb6cDlYrA-"
      },
      "outputs": [],
      "source": [
        "# Fixed random seed to make results as reproducible as possible\n",
        "seed = 42\n",
        "random.seed(seed)\n",
        "os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "np.random.seed(seed)\n",
        "tf.random.set_seed(seed)\n",
        "tf.compat.v1.set_random_seed(seed)\n",
        "batch_size = 32"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U-imelj3bycH"
      },
      "source": [
        "# Data merging\n",
        "\n",
        "> ⚠️ first-time use only!\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a function that takes a directory path and returns a sorted list of file paths for all the files in the directory\n",
        "def folderToPaths(full_img_dir):\n",
        "    # Initialize an empty list to store the file paths\n",
        "    x_paths_list = []\n",
        "    full_img_dir = full_img_dir\n",
        "    # Iterate over the files in the directory and append their paths to the list\n",
        "    for full in os.listdir(full_img_dir):\n",
        "         x_paths_list.append(os.path.join(full_img_dir, full))\n",
        "    # Sort the list alphabetically and return it\n",
        "    x_paths_list.sort()\n",
        "    return x_paths_list"
      ],
      "metadata": {
        "id": "tQy_wm2fHnfB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kQnD-YozhF1U"
      },
      "outputs": [],
      "source": [
        "# Set the paths for the directories containing the original data and the merged data\n",
        "dataset_dir = 'data/original_data'\n",
        "merged_dataset_dir = \"data/merged_data\"\n",
        "\n",
        "# Define a list of labels associated with different species\n",
        "labels = ['Species1',       # 0\n",
        "          'Species2',       # 1\n",
        "          'Species3',       # 2\n",
        "          'Species4',       # 3\n",
        "          'Species5',       # 4\n",
        "          'Species6',       # 5\n",
        "          'Species7',       # 6\n",
        "          'Species8'        # 7\n",
        "]\n",
        "\n",
        "# merging all the images, ! one time use only !\n",
        "for idx, class_label in enumerate(labels) :\n",
        "  paths = folderToPaths(full_img_dir = '{}/{}/'.format(dataset_dir, class_label))\n",
        "  \n",
        "  for path in paths:\n",
        "    s = str(idx) + \"_\" + path[-9:]\n",
        "    new_path = os.path.join(merged_dataset_dir, s)\n",
        "    shutil.copy(path, new_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MqNsEUCchhSj"
      },
      "source": [
        "# Data splitting - train/val/test version"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gJoEiGvNa6nL"
      },
      "outputs": [],
      "source": [
        "paths_merged = folderToPaths(merged_dataset_dir)\n",
        "labels_of_paths = [p[-11] for p in paths_merged] # get all labels from the image path\n",
        "\n",
        "validation_percentage = 0.15\n",
        "test_percentage = 0.2\n",
        "\n",
        "X_train_val, X_test = train_test_split(paths_merged, test_size = test_percentage, shuffle = True, stratify = labels_of_paths)\n",
        "\n",
        "labels_of_paths_train_val = [p[-11] for p in X_train_val]\n",
        "\n",
        "X_train, X_val = train_test_split(X_train_val, test_size = validation_percentage, shuffle = True, stratify = labels_of_paths_train_val)\n",
        "\n",
        "labels_train = [p[-11] for p in X_train]\n",
        "labels_test = [p[-11] for p in X_test]\n",
        "labels_val =  [p[-11] for p in X_val]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1dOEjdtUcIpw"
      },
      "source": [
        "## Moving the file into the folders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZkIs6i_wbhS9"
      },
      "outputs": [],
      "source": [
        "import shutil\n",
        "from tqdm import tqdm\n",
        "import glob\n",
        "\n",
        "\n",
        "for root, subdirectories, fi in os.walk(\"data/test/\"):\n",
        "    for sub in subdirectories:\n",
        "        files = os.listdir(root+sub)\n",
        "        for f in files:\n",
        "            os.remove(root+sub+\"/\"+f)\n",
        "for root, subdirectories, fi in os.walk(\"data/train/\"):\n",
        "    for sub in subdirectories:\n",
        "        files = os.listdir(root+sub)\n",
        "        for f in files:\n",
        "            os.remove(root+sub+\"/\"+f)\n",
        "for root, subdirectories, fi in os.walk(\"data/val/\"):\n",
        "    for sub in subdirectories:\n",
        "        files = os.listdir(root+sub)\n",
        "        for f in files:\n",
        "            os.remove(root+sub+\"/\"+f)\n",
        "\n",
        "for img in tqdm(zip(labels_train,X_train),total=len(labels_train)):\n",
        "  shutil.copy(img[1],\"data/train/\"+str(int(img[0])+1))\n",
        "\n",
        "for img in tqdm(zip(labels_test,X_test),total=len(labels_test)):\n",
        "  shutil.copy(img[1],\"data/test/\"+str(int(img[0])+1))\n",
        "\n",
        "for img in tqdm(zip(labels_val,X_val),total=len(labels_val)):\n",
        "  shutil.copy(img[1],\"data/val/\"+str(int(img[0])+1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6mPh1IeEcOGt"
      },
      "source": [
        "## Keras data loader with test set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X_B_aFBPf_B3"
      },
      "outputs": [],
      "source": [
        "training_dir = \"data/train\"\n",
        "validation_dir = \"data/val\"\n",
        "test_dir = \"data/test\"\n",
        "\n",
        "train_data = tf.keras.utils.image_dataset_from_directory(\n",
        "    training_dir,\n",
        "    labels='inferred',\n",
        "    label_mode='categorical',\n",
        "    color_mode='rgb',\n",
        "    batch_size=batch_size,\n",
        "    image_size=(96, 96),\n",
        "    shuffle=True,\n",
        "    seed=seed)\n",
        "\n",
        "val_data = tf.keras.utils.image_dataset_from_directory(\n",
        "    validation_dir,\n",
        "    labels='inferred',\n",
        "    label_mode='categorical',\n",
        "    color_mode='rgb',\n",
        "    batch_size=batch_size,\n",
        "    image_size=(96, 96),\n",
        "    shuffle=True,\n",
        "    seed=seed)\n",
        "\n",
        "test_data = tf.keras.utils.image_dataset_from_directory(\n",
        "    test_dir,\n",
        "    labels='inferred',\n",
        "    label_mode='categorical',\n",
        "    color_mode='rgb',\n",
        "    batch_size=batch_size,\n",
        "    image_size=(96, 96),\n",
        "    shuffle=True,\n",
        "    seed=seed)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LUYrPGEu1jFC"
      },
      "source": [
        "# Data splitting - train/val version"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rY1GE8ytfJTo"
      },
      "outputs": [],
      "source": [
        "paths_merged = folderToPaths(merged_dataset_dir)\n",
        "labels_of_paths = [p[-11] for p in paths_merged]\n",
        "\n",
        "val_percentage = 0.2\n",
        "\n",
        "X_train, X_val = train_test_split(paths_merged, test_size = val_percentage, shuffle = True, stratify = labels_of_paths)\n",
        "\n",
        "labels_train = [p[-11] for p in X_train]\n",
        "labels_val = [p[-11] for p in X_val]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aSsKQHWK1-Av"
      },
      "source": [
        "## Moving the files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GR0-OAbBlCZL",
        "outputId": "0e7ddc0b-0df2-4055-a7cf-7b928c252678"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2832/2832 [10:46<00:00,  4.38it/s]\n",
            "100%|██████████| 709/709 [02:36<00:00,  4.52it/s]\n"
          ]
        }
      ],
      "source": [
        "import shutil\n",
        "from tqdm import tqdm\n",
        "import glob\n",
        "\n",
        "\n",
        "for root, subdirectories, fi in os.walk(\"data/train2/\"):\n",
        "    for sub in subdirectories:\n",
        "        files = os.listdir(root+sub)\n",
        "        for f in files:\n",
        "            os.remove(root+sub+\"/\"+f)\n",
        "for root, subdirectories, fi in os.walk(\"data/val2/\"):\n",
        "    for sub in subdirectories:\n",
        "        files = os.listdir(root+sub)\n",
        "        for f in files:\n",
        "            os.remove(root+sub+\"/\"+f)\n",
        "\n",
        "for img in tqdm(zip(labels_train,X_train),total=len(labels_train)):\n",
        "  shutil.copy(img[1],\"data/train2//\"+str(int(img[0])+1))\n",
        "\n",
        "for img in tqdm(zip(labels_val,X_val),total=len(labels_val)):\n",
        "  shutil.copy(img[1],\"data/val2//\"+str(int(img[0])+1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VPM9vnzm2DSU"
      },
      "source": [
        "## Keras data loader without test set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dF9FhonC1wtC",
        "outputId": "950886c5-0e5b-402d-ad23-808bf18dda98"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 2832 files belonging to 8 classes.\n",
            "Found 709 files belonging to 8 classes.\n"
          ]
        }
      ],
      "source": [
        "training_dir = \"data/train2\"\n",
        "validation_dir = \"data/val2\"\n",
        "\n",
        "train_data = tf.keras.utils.image_dataset_from_directory(\n",
        "    training_dir,\n",
        "    labels='inferred',\n",
        "    label_mode='categorical',\n",
        "    color_mode='rgb',\n",
        "    batch_size=batch_size,\n",
        "    image_size=(96, 96),\n",
        "    shuffle=True,\n",
        "    seed=seed)\n",
        "\n",
        "val_data = tf.keras.utils.image_dataset_from_directory(\n",
        "    validation_dir,\n",
        "    labels='inferred',\n",
        "    label_mode='categorical',\n",
        "    color_mode='rgb',\n",
        "    batch_size=batch_size,\n",
        "    image_size=(96, 96),\n",
        "    shuffle=True,\n",
        "    seed=seed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IX2CnL3RfWT_"
      },
      "source": [
        "# Cyclical Learning Rate"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cyclical learning rate (CLR) is a technique used in deep learning that involves varying the learning rate of the optimizer during training. Instead of using a fixed learning rate throughout the entire training process, the learning rate is gradually increased and then decreased in a cyclical manner. This technique was introduced by Leslie N. Smith in 2015.\n",
        "\n",
        "The idea behind CLR is to allow the model to explore a wider range of learning rates and find the optimal learning rate for the given problem. This is achieved by gradually increasing the learning rate from a lower bound to an upper bound and then decreasing it back to the lower bound. This cycle can be repeated multiple times during the training process.\n",
        "\n",
        "The benefits of CLR include faster convergence to the optimal solution and better generalization performance of the model. It can also help prevent the model from getting stuck in local minima by allowing it to escape to a different part of the loss landscape.\n",
        "\n",
        "CLR can be implemented using various techniques, including triangular learning rate policy, triangular2 learning rate policy, and exponential learning rate policy. These policies differ in how the learning rate is varied over each cycle. The choice of policy depends on the specific problem and the architecture of the model being trained."
      ],
      "metadata": {
        "id": "EIg3JnBYI91L"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sBP1g6XKw4rW"
      },
      "outputs": [],
      "source": [
        "class CyclicLR(Callback):\n",
        "    \"\"\"\n",
        "    code taken from https://github.com/bckenstler/CLR/blob/master/clr_callback.py\n",
        "\n",
        "    This callback implements a cyclical learning rate policy (CLR).\n",
        "    The method cycles the learning rate between two boundaries with\n",
        "    some constant frequency, as detailed in this paper (https://arxiv.org/abs/1506.01186).\n",
        "    The amplitude of the cycle can be scaled on a per-iteration or \n",
        "    per-cycle basis.\n",
        "    This class has three built-in policies, as put forth in the paper.\n",
        "    \"triangular\":\n",
        "        A basic triangular cycle w/ no amplitude scaling.\n",
        "    \"triangular2\":\n",
        "        A basic triangular cycle that scales initial amplitude by half each cycle.\n",
        "    \"exp_range\":\n",
        "        A cycle that scales initial amplitude by gamma**(cycle iterations) at each \n",
        "        cycle iteration.\n",
        "    For more detail, please see paper.\n",
        "    \n",
        "    # Example\n",
        "        ```python\n",
        "            clr = CyclicLR(base_lr=0.001, max_lr=0.006,\n",
        "                                step_size=2000., mode='triangular')\n",
        "            model.fit(X_train, Y_train, callbacks=[clr])\n",
        "        ```\n",
        "    \n",
        "    Class also supports custom scaling functions:\n",
        "        ```python\n",
        "            clr_fn = lambda x: 0.5*(1+np.sin(x*np.pi/2.))\n",
        "            clr = CyclicLR(base_lr=0.001, max_lr=0.006,\n",
        "                                step_size=2000., scale_fn=clr_fn,\n",
        "                                scale_mode='cycle')\n",
        "            model.fit(X_train, Y_train, callbacks=[clr])\n",
        "        ```    \n",
        "    # Arguments\n",
        "        base_lr: initial learning rate which is the\n",
        "            lower boundary in the cycle.\n",
        "        max_lr: upper boundary in the cycle. Functionally,\n",
        "            it defines the cycle amplitude (max_lr - base_lr).\n",
        "            The lr at any cycle is the sum of base_lr\n",
        "            and some scaling of the amplitude; therefore \n",
        "            max_lr may not actually be reached depending on\n",
        "            scaling function.\n",
        "        step_size: number of training iterations per\n",
        "            half cycle. Authors suggest setting step_size\n",
        "            2-8 x training iterations in epoch.\n",
        "        mode: one of {triangular, triangular2, exp_range}.\n",
        "            Default 'triangular'.\n",
        "            Values correspond to policies detailed above.\n",
        "            If scale_fn is not None, this argument is ignored.\n",
        "        gamma: constant in 'exp_range' scaling function:\n",
        "            gamma**(cycle iterations)\n",
        "        scale_fn: Custom scaling policy defined by a single\n",
        "            argument lambda function, where \n",
        "            0 <= scale_fn(x) <= 1 for all x >= 0.\n",
        "            mode paramater is ignored \n",
        "        scale_mode: {'cycle', 'iterations'}.\n",
        "            Defines whether scale_fn is evaluated on \n",
        "            cycle number or cycle iterations (training\n",
        "            iterations since start of cycle). Default is 'cycle'.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, base_lr=0.001, max_lr=0.006, step_size=2000., mode='triangular',\n",
        "                 gamma=1., scale_fn=None, scale_mode='cycle'):\n",
        "        super(CyclicLR, self).__init__()\n",
        "\n",
        "        self.base_lr = base_lr\n",
        "        self.max_lr = max_lr\n",
        "        self.step_size = step_size\n",
        "        self.mode = mode\n",
        "        self.gamma = gamma\n",
        "        if scale_fn == None:\n",
        "            if self.mode == 'triangular':\n",
        "                self.scale_fn = lambda x: 1.\n",
        "                self.scale_mode = 'cycle'\n",
        "            elif self.mode == 'triangular2':\n",
        "                self.scale_fn = lambda x: 1/(2.**(x-1))\n",
        "                self.scale_mode = 'cycle'\n",
        "            elif self.mode == 'exp_range':\n",
        "                self.scale_fn = lambda x: gamma**(x)\n",
        "                self.scale_mode = 'iterations'\n",
        "        else:\n",
        "            self.scale_fn = scale_fn\n",
        "            self.scale_mode = scale_mode\n",
        "        self.clr_iterations = 0.\n",
        "        self.trn_iterations = 0.\n",
        "        self.history = {}\n",
        "        self._reset()\n",
        "\n",
        "    def _reset(self, new_base_lr=None, new_max_lr=None,\n",
        "               new_step_size=None):\n",
        "        \"\"\"Resets cycle iterations.\n",
        "        Optional boundary/step size adjustment.\n",
        "        \"\"\"\n",
        "        if new_base_lr != None:\n",
        "            self.base_lr = new_base_lr\n",
        "        if new_max_lr != None:\n",
        "            self.max_lr = new_max_lr\n",
        "        if new_step_size != None:\n",
        "            self.step_size = new_step_size\n",
        "        self.clr_iterations = 0.\n",
        "        \n",
        "    def clr(self):\n",
        "        cycle = np.floor(1+self.clr_iterations/(2*self.step_size))\n",
        "        x = np.abs(self.clr_iterations/self.step_size - 2*cycle + 1)\n",
        "        if self.scale_mode == 'cycle':\n",
        "            return self.base_lr + (self.max_lr-self.base_lr)*np.maximum(0, (1-x))*self.scale_fn(cycle)\n",
        "        else:\n",
        "            return self.base_lr + (self.max_lr-self.base_lr)*np.maximum(0, (1-x))*self.scale_fn(self.clr_iterations)\n",
        "        \n",
        "    def on_train_begin(self, logs={}):\n",
        "        logs = logs or {}\n",
        "\n",
        "        if self.clr_iterations == 0:\n",
        "            K.set_value(self.model.optimizer.lr, self.base_lr)\n",
        "        else:\n",
        "            K.set_value(self.model.optimizer.lr, self.clr())        \n",
        "            \n",
        "    def on_batch_end(self, epoch, logs=None):\n",
        "        \n",
        "        logs = logs or {}\n",
        "        self.trn_iterations += 1\n",
        "        self.clr_iterations += 1\n",
        "\n",
        "        self.history.setdefault('lr', []).append(K.get_value(self.model.optimizer.lr))\n",
        "        self.history.setdefault('iterations', []).append(self.trn_iterations)\n",
        "\n",
        "        for k, v in logs.items():\n",
        "            self.history.setdefault(k, []).append(v)\n",
        "        \n",
        "        K.set_value(self.model.optimizer.lr, self.clr())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "KPaKLzqqH7D4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Final Model"
      ],
      "metadata": {
        "id": "u1UzfrccH_oA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Supernet instantiation"
      ],
      "metadata": {
        "id": "-mtssbQeIDTf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yYGPKHREqufW"
      },
      "outputs": [],
      "source": [
        "supernet = tf.keras.applications.convnext.ConvNeXtLarge(\n",
        "    include_top=False,\n",
        "    weights=\"imagenet\",\n",
        "    input_shape=(224,224,3)\n",
        ")\n",
        "supernet.summary()\n",
        "\n",
        "\n",
        "count = 1\n",
        "for layer in supernet.layers:\n",
        "    if count < 88:\n",
        "        layer.trainable = False\n",
        "    else:\n",
        "        layer.trainable = True\n",
        "    count = count + 1"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Final model building"
      ],
      "metadata": {
        "id": "qDOwKwmgINbW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i-Mkoyd5_WU5"
      },
      "outputs": [],
      "source": [
        "import keras_cv #for CutOut agumentation\n",
        "from keras import regularizers\n",
        "\n",
        "inputs = tfk.Input((96,96,3))\n",
        "\n",
        "# RESIZING\n",
        "x = tfkl.Resizing(224,224,interpolation = \"bicubic\")(inputs)\n",
        "\n",
        "# AUGMENTATION\n",
        "x = keras_cv.layers.RandomCutout(0.35, 0.35)(x)\n",
        "\n",
        "x = tf.keras.layers.RandomBrightness(\n",
        "    0.25, value_range=(0, 255), seed=seed\n",
        ")(x)\n",
        "\n",
        "x= tf.keras.layers.RandomFlip(\n",
        "    mode=\"horizontal_and_vertical\", seed=seed\n",
        ")(x)\n",
        "\n",
        "x = tf.keras.layers.RandomTranslation(\n",
        "    (-0.25, 0.25) ,\n",
        "    (-0.25, 0.25) ,\n",
        "    fill_mode=\"reflect\",\n",
        "    interpolation=\"bilinear\",\n",
        "    seed=seed\n",
        ")(x)\n",
        "\n",
        "x = tf.keras.layers.RandomRotation(\n",
        "    (-0.25, 0.25),\n",
        "    fill_mode=\"reflect\",\n",
        "    interpolation=\"bilinear\",\n",
        "    seed=seed\n",
        ")(x)\n",
        "\n",
        "x = tf.keras.layers.RandomContrast(0.2, seed=seed)(x)\n",
        "\n",
        "x = supernet(x)\n",
        "x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
        "x = tf.keras.layers.Dropout(0.3)(x)\n",
        "x = tf.keras.layers.Dense(1024, activation='relu', kernel_initializer = tfk.initializers.HeUniform(seed),\n",
        "                          kernel_regularizer=regularizers.L1L2(l1=1e-5, l2=1e-4),\n",
        "                          bias_regularizer=regularizers.L2(1e-4),\n",
        "                          activity_regularizer=regularizers.L2(1e-4))(x)\n",
        "outputs = tf.keras.layers.Dense(8, activation='softmax', kernel_initializer = tfk.initializers.GlorotUniform(seed))(x)\n",
        "\n",
        "\n",
        "model = tfk.Model(inputs=inputs, outputs=outputs, name='model')\n",
        "model.compile(loss=tfk.losses.CategoricalCrossentropy(), optimizer=tfk.optimizers.Adam(learning_rate = 1e-4), metrics='accuracy')\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H2S0WlRjxy7O"
      },
      "outputs": [],
      "source": [
        "training_samples = int(len(train_data)*batch_size)\n",
        "step_size = 5*training_samples // batch_size\n",
        "\n",
        "clr = CyclicLR(\n",
        "    mode='triangular',\n",
        "    base_lr=1e-5, \n",
        "    max_lr=1e-4,\n",
        "    step_size= step_size)\n",
        "\n",
        "history = model.fit(\n",
        "    train_data,\n",
        "    epochs = 500,\n",
        "    validation_data = val_data,\n",
        "    callbacks = [tfk.callbacks.EarlyStopping(monitor='val_accuracy', mode='max', patience=8, restore_best_weights=True),clr]\n",
        ").history"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Accordingly to what we stated in the report we decided to modify the Learning rate during the training. However, we did this by hand. One of the possible future works could be to implement a proper LR Scheduler."
      ],
      "metadata": {
        "id": "FPSSJ-xcHeZM"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KyVBU0bVx4zl"
      },
      "source": [
        "In order to be able to submit our model to CodaLab we performed a little trick since keras_cv was not present in the test enviroment: we crafted another identical model with the same structure except the cutout layer, then we copied the weights from the complete one."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N4Svc5hiZH80"
      },
      "outputs": [],
      "source": [
        "supernet1 = tf.keras.applications.convnext.ConvNeXtLarge(\n",
        "    include_top=False,\n",
        "    weights=\"imagenet\",\n",
        "    input_shape=(224,224,3)\n",
        ")\n",
        "\n",
        "\n",
        "count = 1\n",
        "for layer in supernet1.layers:\n",
        "    if count < 88:\n",
        "        layer.trainable = False\n",
        "    else:\n",
        "        layer.trainable = True\n",
        "    count = count + 1"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.layers import Dense,Flatten,GlobalAveragePooling2D, MaxPooling2D, BatchNormalization,Concatenate\n",
        "from keras import regularizers\n",
        "\n",
        "inputs = tfk.Input((96,96,3))\n",
        "\n",
        "# RESIZING\n",
        "x = tfkl.Resizing(224,224,interpolation = \"bicubic\")(inputs)\n",
        "\n",
        "# AUGMENTATION\n",
        "x = tf.keras.layers.RandomBrightness(\n",
        "    0.25, value_range=(0, 255), seed=seed\n",
        ")(x)\n",
        "\n",
        "x= tf.keras.layers.RandomFlip(\n",
        "    mode=\"horizontal_and_vertical\", seed=seed\n",
        ")(x)\n",
        "\n",
        "x = tf.keras.layers.RandomTranslation(\n",
        "    (-0.25, 0.25) ,\n",
        "    (-0.25, 0.25) ,\n",
        "    fill_mode=\"reflect\",\n",
        "    interpolation=\"bilinear\",\n",
        "    seed=seed\n",
        ")(x)\n",
        "\n",
        "x = tf.keras.layers.RandomRotation(\n",
        "    (-0.25, 0.25),\n",
        "    fill_mode=\"reflect\",\n",
        "    interpolation=\"bilinear\",\n",
        "    seed=seed\n",
        ")(x)\n",
        "\n",
        "x = tf.keras.layers.RandomContrast(0.2, seed=seed)(x)\n",
        "\n",
        "x = supernet1(x)\n",
        "x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
        "x = tf.keras.layers.Dropout(0.3)(x)\n",
        "x = tf.keras.layers.Dense(1024, activation='relu', kernel_initializer = tfk.initializers.HeUniform(seed),\n",
        "                          kernel_regularizer=regularizers.L1L2(l1=1e-5, l2=1e-4),\n",
        "                          bias_regularizer=regularizers.L2(1e-4),\n",
        "                          activity_regularizer=regularizers.L2(1e-4))(x)\n",
        "\n",
        "outputs = tf.keras.layers.Dense(8, activation='softmax', kernel_initializer = tfk.initializers.GlorotUniform(seed))(x)\n",
        "\n",
        "# Connect input and output through the Model class\n",
        "model2 = tfk.Model(inputs=inputs, outputs=outputs, name='model')\n",
        "\n",
        "model2.compile(loss=tfk.losses.CategoricalCrossentropy(), optimizer=tfk.optimizers.Adam(learning_rate = 1e-4), metrics='accuracy')\n",
        "\n",
        "model2.summary()"
      ],
      "metadata": {
        "id": "feL0kj21S3TO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6a4ba682-fb85-443d-9773-9f0e36ccf0c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_4 (InputLayer)        [(None, 96, 96, 3)]       0         \n",
            "                                                                 \n",
            " resizing_1 (Resizing)       (None, 224, 224, 3)       0         \n",
            "                                                                 \n",
            " random_brightness_1 (Random  (None, 224, 224, 3)      0         \n",
            " Brightness)                                                     \n",
            "                                                                 \n",
            " random_flip_1 (RandomFlip)  (None, 224, 224, 3)       0         \n",
            "                                                                 \n",
            " random_translation_1 (Rando  (None, 224, 224, 3)      0         \n",
            " mTranslation)                                                   \n",
            "                                                                 \n",
            " random_rotation_1 (RandomRo  (None, 224, 224, 3)      0         \n",
            " tation)                                                         \n",
            "                                                                 \n",
            " random_contrast_1 (RandomCo  (None, 224, 224, 3)      0         \n",
            " ntrast)                                                         \n",
            "                                                                 \n",
            " convnext_large (Functional)  (None, 7, 7, 1536)       196230336 \n",
            "                                                                 \n",
            " global_average_pooling2d_1   (None, 1536)             0         \n",
            " (GlobalAveragePooling2D)                                        \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 1536)              0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 1024)              1573888   \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 8)                 8200      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 197,812,424\n",
            "Trainable params: 172,709,384\n",
            "Non-trainable params: 25,103,040\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model2.set_weights(model.get_weights()) # Final model for Codalab submission"
      ],
      "metadata": {
        "id": "fh0D_0HtS70k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model2.save(\"model.h5\")"
      ],
      "metadata": {
        "id": "IxJuT5tkEWbw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Prediction"
      ],
      "metadata": {
        "id": "tbhFXfmrQJ_M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We used Test Time Augmentation in our model inference to improve our accuracy\n",
        "\n",
        "Test Time Augmentation (TTA) is a technique used in machine learning and computer vision to improve the accuracy of a trained model's predictions. TTA involves applying data augmentation techniques to test images, in addition to the original image, and then taking the average or maximum of the model's predictions on all the augmented images to make a final prediction.\n",
        "\n",
        "The idea behind TTA is to introduce more variety into the test set to account for variations in the input images that were not present in the training set. By applying different types of transformations to the test images, such as rotation, scaling, or cropping, the model is forced to make more robust predictions and handle variations in the input data."
      ],
      "metadata": {
        "id": "jBo-P8VyP4ZR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "yQLcAq1jK-ZA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def flip_lr(images):\n",
        "    return np.flip(images, axis=2)\n",
        "\n",
        "def shift(images, shift, axis):\n",
        "    return np.roll(images, shift, axis=axis)\n",
        "\n",
        "def rotate(images, angle):\n",
        "    return sp.ndimage.rotate(\n",
        "        images, angle, axes=(1,2),\n",
        "        reshape=False, mode='nearest')\n",
        "\n",
        "def combine_predictions(predictions):\n",
        "    pred_agg = np.mean(predictions, axis=0)\n",
        "    preds = np.argmax(pred_agg, axis=-1)\n",
        "    return preds\n",
        "\n",
        "def tta_predict(m, x_test):\n",
        "    pred = m.predict(x_test)\n",
        "\n",
        "    pred_f = m.predict(flip_lr(x_test))\n",
        "\n",
        "    pred_w0 = m.predict(shift(x_test, -3, axis=2))\n",
        "    pred_w1 = m.predict(shift(x_test, 3, axis=2))\n",
        "\n",
        "    pred_h0 = m.predict(shift(x_test, -3, axis=1))\n",
        "    pred_h1 = m.predict(shift(x_test, 3, axis=1))\n",
        "\n",
        "    pred_r0 = m.predict(rotate(x_test, -10))\n",
        "    pred_r1 = m.predict(rotate(x_test, 10))\n",
        "    out = combine_predictions(np.stack((pred, pred_h0, pred_h1, pred_w0, pred_w1, pred_f, pred_r0, pred_r1)))\n",
        "    return tf.convert_to_tensor(out)"
      ],
      "metadata": {
        "id": "WmyHdJvhSDWC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = tta_predict(model2, test_data)"
      ],
      "metadata": {
        "id": "yhfkhRBFSMty"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "collapsed_sections": [
        "bVGKQ8OKGqJ8",
        "LPedLrndGv77",
        "FrtZTjW0HAVR",
        "jgNHqEB3HClH",
        "U-imelj3bycH",
        "MqNsEUCchhSj",
        "1dOEjdtUcIpw",
        "6mPh1IeEcOGt",
        "LUYrPGEu1jFC",
        "IX2CnL3RfWT_",
        "u1UzfrccH_oA",
        "-mtssbQeIDTf",
        "qDOwKwmgINbW",
        "tbhFXfmrQJ_M"
      ]
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}